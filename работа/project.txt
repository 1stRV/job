Проект "Markus" Табак
						  
Гос информационная система МОТП(Маркировка оборота табачной продукции).

*Штрих-код(одномерный)
идентицифирует пачку

*QR-код(Quick Responce Code двумерный)
матричный тип штрих-кода

*ОФД(Оператор фискальных данных)
все чеки с кассы уходят в ОФД. 
Например, в приложении Тинькофф можно посмотреть чек и QR-код

Сейчас продается пачка, сканируется QR-код и отправляется в ОФД. ОФД выгружает.

Есть схема общая с системами. А есть конректная схема нашего проекта "Markus"

В магазине есть кассы. Продажа отражается в ОФД. Он их сбрасывает в МОТП. 

Есть еще производители сигарет к нам это не относится, когда они выпускают это тоже сбрасывают в МОТП.

МОТП проверяет что проданная продукция была выпущена поставлена.

Щас тока продажи и выпуск отражаются. В дальнейшем этот оператор МОТП хочет отслеживать все перемещение продукции между разными юрлицами. Мы покупаем не на заводе например, а прежде чем к нам попадает в магазин продукция она проходит через перекупы.

Там тысяча сигарет как отследить продаешь тысяча пачек или две тысячи.

*РЦ(Распределительный центр)Пятерочка, Перекресток, Карусель
склады там стоит свой софт и например поставщик отгружает нам он должен сказать что он отгрузил продукцию такомутому юрлицу АО Перекресток.(у нас 36 юрлиц)

РЦ когда приняло, оно говорит МОТП что эта продукция у нас. РЦ отгружает на магазин. РЦ и магазин могут быть разными юр компаниями.  Эту отгрузку надо тоже зафиксировать в МОТП, что мы передали от одного юрлица другому. 

Потом когда осуществляется продажа идет по той, же цепочке.

Система "Markus" это зеркало МОТП.

*Зеркало веб-сайта
копия одного сервера на другом

МОТП делается потому что все компании на самоокупаемости они содержат себя за счет штрафов, которые выставляются ретаилерам бизнесам, они скажут вы реализуете контрафактную продукцию, у вас ее быть не должно. Чтобы не попадать в неприятные ситуации делаются система "Markus". 

В "Markus" внутри мы будем сами учитывать перемещение. Например, начиная от входа к нам, перемещение между юрлицами, списание, реализация товара, мы должны внутри себя учитывать, эти перемещения будут прогоняться через "Markus" и попадать в МОТП, например, чек ушел в ОФД, и магазин сбросит нам через 

*SAP urp 
бэкофисная система
немецкая 1С дорогая. 
Чеки продаж сбросятся в SAP отдаст нам, мы у себя зафиксируем что товар был реализован. Это 1 из целей. 

Есть более сложная например фонд сведения запутались никто по пачечно не будет принимать. На РЦ приходится 460тыс. пачек в день. 

*Сейчас готово
/1-ый сервис который работает с МОТП. Нету обработчика ошибок, логирования, бизнес-логики. Нам прислали ссылку на работающий МОТП. Макс сделал незавершенный. 

/2-ой сервис master data
мы еще не знаем формат данных, который нам приходит. 

*Микросервисы
они у нас не очень микро, т.к. команда маленькая, и эта хреновена вызывает вызов МОТП 20 функций, и по парадигме микросервисов нам надо 20 микросервисов написать и каждый будет заниматься своей историей. Сейчас разносим по слоям, потом будем разносить по микросервисам. 

Есть МОТП Sender Макс все эти методы пока что в одном классе написал, пометил, что их надо отрефакторить. 

Вызов МОТП например вся логика это ты делаешь GET-запрос получаешь какие-то данные соответственно эта фигня вызывается kafka-Listener ом.

Consumer получает на вход из kafka какой-то запрос и сделал get agregated который просто отправил GET-запрос в МОТП и получил ответ сложил его. 

Это не микросервис так как пока на микровость не заморачивались чтобы они были.

Сейчас они поделены по функционалу.

Задача этого сервиса у него нет бизнес логики он просто обменивается с МОТП, реализует авторизацию, имплементирует в себе и отдает ответ. Это шлюз такой у них. 

Если будут идеи по опыту, то нужно сказать Максу. 

Вот магазины за пределами нашей системы, вот МОТП. Наша задача как-то эту историю проброс сообщений фиксировать у себя. 

МОТП входит не только через нашу систему, потом расскажу.

Есть еще поток когда принимается товар. Он через UPD(универсальный передаточный документ) через IDE провайдер, это гос заморочки, отправляет сюда эту информацию. А нам он скажет, что нам ее отправил. Мы не единственный шлюз в этой системе. 

*Работа системы 
по середине находится Worker Service. (Его потом надо будет пилить на микросервисы)
Пока мы его функционал не очень знаем он у нас один большой сервис.

Worker Service работает с Mongo DB и с Oracle DB(историческая). Это оперативная информация о текущем статусе. Как только статус КИЗ(контрольно-идентифиткационный знак) не актуален тут есть таблица фактов в Oracle DB. Мы планируем скидывать все что нам не надо сейчас в историческую БД, чтобы у нас быстро работала оперативная БД. 

Worker Service несет в себе бизнес логику. Там есть логика как перемещать, что проверять, как сверять, как разносить марки по поставкам, по контрагентам. 

Магазины, склады отправляют запросы.

*Citrix Nite Sceller
маршрутизатор запросов сервис Discovery это ему просто говоришь, что есть такие-то эндпойнты, магазин обращается на 1точку, он знает, что есть эндпойнты, эти сервисы предполгается что горизонтально масштабируются, и он просто разбрасывает входящие запросы между сервисами.

Здесь это Rest API это от него мы уже пересмотрели подход(в понедельник будет встреча расскжут как эта штука работает).

 Изначальн мы предпологали, что это будет Rest API, что ыфзфывают только нас, асинхронно, что нас спросили, мы подумали, сделали, тут должна быть еще одна Mongo DB была. Мы положили готовый ответ когда у нас просят ответ мы проверяем есть ли у нас ответ, отвечаем. Это был щит которым мы хотели разгрузить этот сервис. У нас 16тысяч магазинов, мы прогнозировали, что эта система будет выполнять 7,5милионов операций в день. Количество запросов особенно аинхронных, особенно GET-status 16тыс. магазинов, это могут генерить 30тыс/минуту, 1,8млн/час, в деь 30млн. запросов по 1ому интерфейсу. 
 
 Архитектор сказал REST нам не подходит, чтобы нас не дудосили. Нагружают очень сильно каналы корпоративные. Он предлагают нам, чтобы мы получили запрос, обработали его, а инициировать обратный ответ будем мы c помощью шины API SAP. 
 
*REST будет на вход

*На выход?

Пока рабочий вариант следующий:
Здесь мы поднимаем REST API он получает запрос, единственная его задача трансформировать наше внутреннее представление, например, мы получаем XML неудобный с лишними полями, он его форматирует в JSON и складывает его в kafka. 

*Kafka
работает как очередь, Worker Service забирает эти запросы из kafka обрабатывает. Ответ он складывает в kafka. Здесь есть Listener в этом же сервисе или в другом, и он получает ответ из kafka и просто будет пробрасывать его дальше, это будет не Rest мы будем сами его инициировать соединение с магазином, с теми, кто запросил. Надо будет хранить кому мы отвечаем, на какой запрос. 

Возможно это будет 2сервиса, 1будет REST API на вход, другой сервис мы его назовем Sender или ExceChager, который будет пробрасывать ответы и все. Этого пока нет, чтобы ты что-то написать нужно понять какие тут данные. 

Kafka нужна здесь чтобы делать всю эту историю асинхронно, и  развязываем сервисы мы возлагаем на нее такую бизнес функцию, что мы в нее сложили и все сервис можно потушить и все сервисы должны быть Stateles(не должны хранить состояние они в любой момент могут быть потушены и ниче не помнят, соответственно kafka должна помнить то, что работает).

Worker что-то делает, например, нам пришла поставка, документ такой-то, такие-то строки документа и там куча КИЗов. 

2 БД нужны чтобы для документов мы заголовок документа называется номер документа, название его статус строки мы будем складывать в Oracle(реляционная модель, документ, строки, документ ссылается на род документ, это будет реляционная модель, а вот КИЗы).


*Транспортирока сигарет иерархия
пачка, блок, короб, палет. У всех них есть свой КИЗ.

КИЗ каждого отличается. 

КИЗ блока в системе МОТП знает какие пачки в нем лежат. Под КИЗом короба в МОТП лежат блоки, под КИЗом палета лежат короба. В любой момент может не быть любой из этих вещей. На палет могут быть поставлены блоки. 

КИЗ представляет собой строчку. Если сканировать КИЗ получается строчка. И в программе иерархия КИЗов. В JSON. 

*Есть API МОТП(с ошибками недописанная)
половина мест не заполнено, половина мест содержит неверную инфу. Описанные интерфейсы работают не так, как написано. 

Например, параметр стоит вот так вот, а реально API последнее которое, мы видели там параметр в GET-запросе указан.

Конкретные именнованные параметры GET-запроса. 

Карточка продукт, ?product=46

*Задачи
1)ресерч Mongo DB. 
мы поняли, что в Mongo DB нам надо хранить КИЗы. Нам нужно быстро их забирать и они хранятся по документам. Мы не поняли как связи поддерживать есть DB Listener. Или хранить документами. Документы большие будут. В фуре 460тыс. пачек. 

Макс написал циклы и там 38палетов, 25коробов, 50блоков, 10пачек. В Mongo DB.

Нужно сделать и такую и такую структуру, загнал туда 1000 фур и как быстрее будет искать по КИЗам. Провести экспмеримент и выбрать структуру хранения.

status владельца, status проверили или нет, мы считаем, ято в Mongo храним только текущий статус.  Например, мы не знаем владельца, пишем не знаем владельца, мы проверили, это наше, мы в Mongo меняем владельца, а все что быо до этого мы скидываем в историческую БД(реляционной модели). А предыдущее состояние получаем через Worker. 

2варианта хранения:
1)когда мы List КИЗов делаем, второй вариант когда ссылка на верх. 
Третий вариант ресерч DB ref реляционная ссылка в Mongo. Например, есть КИЗ он ссылается на 321 КИЗ. Самого КИЗа нет и когда ты будешь выборку делать он найде. Когда Макс записал все это в БД. То визуальное отображение зависло. Мбайт получился в палете. 

Надо ресерч какая структура хранения лучше. 
1)иерархическая структура 2)КИЗ ссылкается на родителей. И тогда отдельные КИЗы все. 

Например, задачи:
1)есть большая вложенность найти КИЗ нужно независимо от вложенности первая задача.

2)при изменении статуса КИЗа поменять статусы у всх его родителей.(например, храним короб, блок пачка, как только нам придет инфа о том, что пачка продана, мы понимаем, что больше короба и блока не существует как учетной единицы их распаковали дизагрегировали это называется(агрегация-это собрание)).

При информации о продаже одного из кейсов нам надо родителям проставить признак disagregated. 

*Подумать
проставить disagregated или нам надо разбить все и не хранить больше в структуре.

Для иерархической вещи как мы документами храним или строками храним или ссылками DB refом (это вроде экономит место и не понятно как им управлять). 

*
dDB ref сэкономит место ссылка только по id(описания больше нет) или просто будем строчкой вставлять. 

Дмитрий предлагает взять 20млн пачек записать написать выборки, поиск такой, агргация, посмотреть как она работает. В другом варианте написать поиск, изменения, изменение статусов, добавляется элементы, тоесть были короба, мы запросили информацию о том, какие там блоки, пачки.

Изучить версионность транзакционность Mongo DB, когда update тишь документ.

Например, есть палеты и 1  наш сервис узнал о том, что внутри где-то продали пачку, и в этот же момент другой наш сервис узнал, что продали пачку в другой палет, они оба взяли Entity сохранили и сохранит последний перезапишет. В mongo есть эта фича в Spring. Это операция дороже, не save, другая операция, нужно исследовать.

У документа простовляется версия и он когда сохраняе, когда выбрал он проверяет версию и перед сохранением он лочит изменения и когда сохраняет и если вот эта версия, то сохраняет , а если не эта, то он забирает мержит, изменения или просто удаляет \то надо исследовать. Завести в Confluence.

Макс приложил API, электронно-цифровую подпись кусок кода, описал алгоритмы авторизации, трехшаговое получение запросов, один из алгоритмов обмена с МОТП сервис.

Тот запрашивает документ вот алгоритм обмена. Взаимодействие компонентов изучить по пунктам сократить. Есть потоки данных плохо расписаны откуда куда стрелки не ясно. Стрелочка вправо POST стрелочка влево GET. 

Алгоритм приемки работа с UPD первый процесс который, будем реализовавывать шаг ЭРЦ.

Потом выдадут сервера для разработки. В Confluence завести статью в разделе разработка. Уже описан Doker образ с Kafka, Redis. Начали писать правила логирования описываем, что пишем в Trace, Debug.

В GitLab уже написаны МОТП Sender утилита обмена, Markus data это сервис для мастер данных, МОТП Exchanger Test Client(протестирован обмен через Kafka он отправляет запрос, получает ответ, токен он загружал, мы руками его получали через docker compose потом загружали и сутки можно работать)

Изучить Kafka не в Confluence сделать себе конспект.


Redis все это горизонт масштабируется. Есть МОТП там авторизация по электронно-цифровой подписи, взаимодействие с КриптоПро. Есть библиотека Bounty Kasta берет все виды ключей бесплатно RCH шные даже. Если это ключ КриптоПро, то Bounty Kasta его не понимает.(Это не нужно)

Рабочая схема
ты  обращаешься в МОТП H они тебе отвечают рандомной строкой ты должен рандомную строку подписать которая зарегистровано на твое юрлицо (эта штука подписывается есть билиотека КриптоПро у нас будет сервис внешний мы будем обращаться к нему и авторизовываться подпишет и в ответ дают токен, дальше мы работаем по токену, Redis прикрутили для того, чтобы расшарить токен между сервисами далее нарпимер надо будет новую инфу расшарить тоже используем Redis и мы не прикручивали сюда никаких БД и логики, а просто Redis нужен чтобы хранить токен) если вдруг  токен просрочился какой-то сервис увидел,  полез авторизовался сложил токен в Redis. 

*Сервис отчетности 
будет работать с большой исторической БД пока так. Потом примут архитектурное решение. 

*Сервис Master Data
все мастер данные перечень магазинов, юрлиц, контрагенты, (Stored Point Price Loockup, перечень штрихкодов которые идентифицируют, это все термины международные)id товара, 

*МРЗ(минимальная розничная цена)
 максимальная цена по которой можно продавать сигареты. 

Если МРЦ меняется, то штрих-код меняется из SAPа в Master Data придут новые данные.

Rest сервис его вызовет SAP и скажет вот тебе новые Master данные их положит.

Заглушка сейчас уже нарисована. Задача получить сохранить в БД.

Сергей сейчас напишет нам данные и уже есть в GitLab готовый сервис там несколько строк всего. 

 *Config 
пока что нет отдельного сервиса он тока на диаграмме, а в проекте конфиги все в одной папке много файлов application.properties так как админы сказали что приложения должны конфигурироватсья внутри. У нас в компании не используется Config Service.

Вся конфигурация будет на докерах. 

Есть DTO много. Этот сервис будет складывать сюда и этому сервису надо получать эти DTO. Чтобы не писать конвертеры т.к. когда работаешь с Kafka она по-умолчанию она с помощью Spring проверяет целиком тип данных класс, пакети эти DTO надо вынести в некую клиентскую часть библиотеку которая зависимость подтягивается и туда  и туда чтобы конвертер не делпать. Сделать интерфейсы с DTO чтобы просто здесь имплементировать.  Такая же будет вот здесь. 

Kafka в кластере. Сервисы горизонтально масштабирцются, Mongo в кластере. Выбрали Mongo т.к. нагрузка будет большая и должно легко масштабироваться. 

Postgres, Oracle тяжело масштабируются.

Mongo DB масштабируется просто из коробки. Ты кластер Mongo делаешь и она шаргируется, правило шаргинга, например, по КИЗам будем шаргить. 

В кластере Kafka по партициям разбивается по Consumer.

Партиционаруется есть понятие key. И соответствует партицие. одну партицию слушает не более 1 Consumer.

*Партиция 
в Kafka топик. 

*Топик Kafka
он делится по партициям.

Надо быстро разгребать очереди ты поднимаешь десяток клиентов, то очередь не будет разгребаться. 1 клиент по-прежнему будет с ней работать тебе надо разбить на 10 партиций в Kafka.

Тогда клиенты Consumer называются разбирутся по партициям. 

ЕЛК для анализа логов со всей 




 

 






*  class TokenRequest Подлписанные данные для авторизации в ИС МОТП из
* {@link ru.x5.motpsender.dao.dto.AuthResponse} подписывается с помощью ЭЦП







